{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0affeae5",
   "metadata": {},
   "source": [
    "Section 1: Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7266d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcea32",
   "metadata": {},
   "source": [
    "Section 2: Load the Cleaned PR Data per Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383642a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directories\n",
    "bipartite_dir = Path(\"../data/networks/bipartite\")\n",
    "projected_dir = Path(\"../data/networks/projected\")\n",
    "\n",
    "bipartite_dir.mkdir(parents=True, exist_ok=True)\n",
    "projected_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Repo paths\n",
    "repo_files = {\n",
    "    \"scikit-learn\": \"../data/processed/cleaned/scikit_learn_cleaned.csv\",\n",
    "    \"pytorch\": \"../data/processed/cleaned/pytorch_cleaned.csv\",\n",
    "    \"kubernetes\": \"../data/processed/cleaned/kubernetes_cleaned.csv\",\n",
    "    \"apache-spark\": \"../data/processed/cleaned/apache_spark_cleaned.csv\"\n",
    "}\n",
    "\n",
    "# Parse contributor lists\n",
    "def parse_contributors(val):\n",
    "    try:\n",
    "        outer = literal_eval(val)\n",
    "        contributors = []\n",
    "        for item in outer:\n",
    "            if isinstance(item, str):\n",
    "                try:\n",
    "                    inner = literal_eval(item)\n",
    "                    if isinstance(inner, list):\n",
    "                        contributors.extend(inner)\n",
    "                    else:\n",
    "                        contributors.append(inner)\n",
    "                except:\n",
    "                    contributors.append(item)\n",
    "            elif isinstance(item, list):\n",
    "                contributors.extend(item)\n",
    "            else:\n",
    "                contributors.append(item)\n",
    "        return [c for c in contributors if isinstance(c, str) and c.strip() and c != \"[]\"]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fbc1b",
   "metadata": {},
   "source": [
    "Section 3: Create the Bipartite Graphs per Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ccfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_bipartite_and_project(df, repo, phase):\n",
    "    df_phase = df[df[\"covid_phase\"] == phase]\n",
    "    print(f\"Processing {repo} ({phase}) – {len(df_phase)} rows\")\n",
    "\n",
    "    B = nx.Graph()\n",
    "    skipped_rows = 0\n",
    "\n",
    "    for _, row in df_phase.iterrows():\n",
    "        pr_id = f\"{repo}_PR_{row['pr_number']}\"\n",
    "        contributors = row[\"all_contributors\"]\n",
    "        if not contributors:\n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "        B.add_node(pr_id, bipartite=0)\n",
    "        for user in contributors:\n",
    "            if pd.notnull(user):\n",
    "                B.add_node(user, bipartite=1)\n",
    "                B.add_edge(user, pr_id)\n",
    "\n",
    "    print(f\"  Bipartite graph has {B.number_of_nodes()} nodes, {B.number_of_edges()} edges. Skipped rows: {skipped_rows}\")\n",
    "\n",
    "    if B.number_of_nodes() == 0:\n",
    "        print(f\"  Skipping {repo} ({phase}) – bipartite graph is empty.\")\n",
    "        return\n",
    "\n",
    "    # Save bipartite graph\n",
    "    bipartite_path = bipartite_dir / f\"{repo}_{phase}_bipartite.gml\"\n",
    "    nx.write_gml(B, bipartite_path)\n",
    "\n",
    "    # Project to developer graph\n",
    "    developers = {n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 1}\n",
    "\n",
    "    if len(developers) == 0 or len(developers) >= len(B):\n",
    "        print(f\"  Skipping projection – invalid developer set (|developers| = {len(developers)}, |B| = {len(B)})\")\n",
    "        return\n",
    "\n",
    "    G = bipartite.weighted_projected_graph(B, developers)\n",
    "\n",
    "    # Threshold edges and remove isolates\n",
    "    G = nx.Graph(((u, v, d) for u, v, d in G.edges(data=True) if d[\"weight\"] > 1))\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    # Save projected graph\n",
    "    projected_path = projected_dir / f\"{repo}_{phase}_projected.gml\"\n",
    "    nx.write_gml(G, projected_path)\n",
    "\n",
    "    print(f\"  Projected graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f144d",
   "metadata": {},
   "source": [
    "Section 4: Save Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59dadf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCIKIT-LEARN ===\n",
      "Processing scikit-learn (pre) – 3436 rows\n",
      "  Bipartite graph has 3474 nodes, 3249 edges. Skipped rows: 1083\n",
      "  Projected graph: 95 nodes, 138 edges\n",
      "Processing scikit-learn (during) – 6211 rows\n",
      "  Bipartite graph has 5964 nodes, 5673 edges. Skipped rows: 1829\n",
      "  Projected graph: 131 nodes, 189 edges\n",
      "Processing scikit-learn (post) – 1085 rows\n",
      "  Bipartite graph has 1081 nodes, 1089 edges. Skipped rows: 298\n",
      "  Projected graph: 35 nodes, 52 edges\n",
      "\n",
      "=== PYTORCH ===\n",
      "Processing pytorch (pre) – 18264 rows\n",
      "  Bipartite graph has 14289 nodes, 33882 edges. Skipped rows: 5342\n",
      "  Projected graph: 359 nodes, 2498 edges\n",
      "Processing pytorch (during) – 46773 rows\n",
      "  Bipartite graph has 38398 nodes, 95945 edges. Skipped rows: 11099\n",
      "  Projected graph: 803 nodes, 8090 edges\n",
      "Processing pytorch (post) – 10265 rows\n",
      "  Bipartite graph has 9208 nodes, 30190 edges. Skipped rows: 2074\n",
      "  Projected graph: 365 nodes, 4539 edges\n",
      "\n",
      "=== KUBERNETES ===\n",
      "Processing kubernetes (pre) – 19803 rows\n",
      "  Bipartite graph has 21956 nodes, 85308 edges. Skipped rows: 17\n",
      "  Projected graph: 1160 nodes, 18491 edges\n",
      "Processing kubernetes (during) – 20341 rows\n",
      "  Bipartite graph has 23114 nodes, 88507 edges. Skipped rows: 26\n",
      "  Projected graph: 1430 nodes, 18550 edges\n",
      "Processing kubernetes (post) – 3213 rows\n",
      "  Bipartite graph has 4049 nodes, 14673 edges. Skipped rows: 2\n",
      "  Projected graph: 423 nodes, 4072 edges\n",
      "\n",
      "=== APACHE-SPARK ===\n",
      "Processing apache-spark (pre) – 7229 rows\n",
      "  Bipartite graph has 5381 nodes, 6556 edges. Skipped rows: 2771\n",
      "  Projected graph: 172 nodes, 388 edges\n",
      "Processing apache-spark (during) – 13654 rows\n",
      "  Bipartite graph has 7535 nodes, 8354 edges. Skipped rows: 7245\n",
      "  Projected graph: 202 nodes, 338 edges\n",
      "Processing apache-spark (post) – 3475 rows\n",
      "  Bipartite graph has 1996 nodes, 2151 edges. Skipped rows: 1879\n",
      "  Projected graph: 76 nodes, 103 edges\n",
      "\n",
      "Saved summary to ../data/results/network_summary.csv\n"
     ]
    }
   ],
   "source": [
    "phases = [\"pre\", \"during\", \"post\"]\n",
    "network_summary = []\n",
    "\n",
    "for repo, path in repo_files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"all_contributors\"] = df[\"all_contributors\"].apply(parse_contributors)\n",
    "    df = df.dropna(subset=[\"covid_phase\"])\n",
    "    print(f\"=== {repo.upper()} ===\")\n",
    "    \n",
    "    for phase in phases:\n",
    "        G = build_bipartite_and_project(df, repo, phase)\n",
    "        \n",
    "        # Only store if the graph was returned\n",
    "        if G is not None:\n",
    "            network_summary.append({\n",
    "                \"Repository\": repo,\n",
    "                \"Phase\": phase,\n",
    "                \"Projected Nodes\": len(G.nodes),\n",
    "                \"Projected Edges\": len(G.edges),\n",
    "            })\n",
    "    print()\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "summary_df = pd.DataFrame(network_summary)\n",
    "summary_df.to_csv(\"../data/results/network_summary.csv\", index=False)\n",
    "print(\"Saved summary to ../data/results/network_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14467160",
   "metadata": {},
   "source": [
    "Section 5: Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fc3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to projected networks\n",
    "projected_dir = Path(\"data/networks/projected\")\n",
    "repos = [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]\n",
    "phases = [\"pre\", \"during\", \"post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcccd64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect node/edge counts\n",
    "summary_data = []\n",
    "\n",
    "for repo in repos:\n",
    "    for phase in phases:\n",
    "        graph_path = projected_dir / f\"{repo}_{phase}_projected.gml\"\n",
    "        if graph_path.exists():\n",
    "            G = nx.read_gml(graph_path)\n",
    "            summary_data.append({\n",
    "                \"repository\": repo,\n",
    "                \"phase\": phase,\n",
    "                \"nodes\": G.number_of_nodes(),\n",
    "                \"edges\": G.number_of_edges()\n",
    "            })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179bf00",
   "metadata": {},
   "source": [
    "Section 6: Visualizing the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c55295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "phases = [\"pre\", \"during\", \"post\"]\n",
    "repos = [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]\n",
    "projected_dir = Path(\"../data/networks/projected\")\n",
    "\n",
    "# Load all graphs\n",
    "graphs_by_phase = {}\n",
    "for repo in repos:\n",
    "    for phase in phases:\n",
    "        graph_path = projected_dir / f\"{repo}_{phase}_projected.gml\"\n",
    "        if graph_path.exists():\n",
    "            G = nx.read_gml(graph_path)\n",
    "            key = f\"{repo}_{phase}\"\n",
    "            graphs_by_phase[key] = G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1674a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"../figures/networks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Visualize and save subgraphs\n",
    "for key, G in graphs_by_phase.items():\n",
    "    if G.number_of_nodes() > 150:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G_sub = G.subgraph(largest_cc).copy()\n",
    "    else:\n",
    "        G_sub = G\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    pos = nx.spring_layout(G_sub, seed=42)\n",
    "    degrees = dict(G_sub.degree)\n",
    "    node_sizes = [v * 8 for v in degrees.values()]\n",
    "    nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes, alpha=0.7, node_color=\"steelblue\")\n",
    "    nx.draw_networkx_edges(G_sub, pos, alpha=0.3)\n",
    "    plt.title(f\"Collaboration Network – {key.replace('_', ' ').title()}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    filename = f\"{key}_network.png\".replace(\" \", \"_\")\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()  # Close to avoid inline rendering in notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875796b",
   "metadata": {},
   "source": [
    "Section 7: Computing Node Strength Across All Projected Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259ffb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      repository   phase  num_nodes  num_edges  mean_strength  \\\n",
      "10  apache-spark  during        202        338          11.37   \n",
      "11  apache-spark    post         76        103           9.37   \n",
      "9   apache-spark     pre        172        388          20.49   \n",
      "7     kubernetes  during       1430      18550         211.44   \n",
      "8     kubernetes    post        423       4072         119.91   \n",
      "6     kubernetes     pre       1160      18491         262.67   \n",
      "4        pytorch  during        803       8090         474.11   \n",
      "5        pytorch    post        365       4539         586.41   \n",
      "3        pytorch     pre        359       2498         317.14   \n",
      "1   scikit-learn  during        131        189           9.31   \n",
      "2   scikit-learn    post         35         52           9.77   \n",
      "0   scikit-learn     pre         95        138           9.07   \n",
      "\n",
      "    median_strength  max_strength  std_strength  \n",
      "10              5.0           119         17.54  \n",
      "11              6.0            64         11.71  \n",
      "9               5.0           255         38.31  \n",
      "7              15.0         13470        735.75  \n",
      "8              21.0          2246        257.02  \n",
      "6              20.0         10686        776.31  \n",
      "4              20.0         24603       2065.01  \n",
      "5              24.0         10229       1718.19  \n",
      "3              17.0         12240       1459.36  \n",
      "1               3.0           138         19.97  \n",
      "2               5.0            64         14.37  \n",
      "0               3.0           124         18.81  \n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "repos = [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]\n",
    "phases = [\"pre\", \"during\", \"post\"]\n",
    "projected_dir = Path(\"../data/networks/projected\")\n",
    "\n",
    "# Storage for summary\n",
    "strength_summary = []\n",
    "\n",
    "# Iterate over all graphs\n",
    "for repo in repos:\n",
    "    for phase in phases:\n",
    "        graph_path = projected_dir / f\"{repo}_{phase}_projected.gml\"\n",
    "        if not graph_path.exists():\n",
    "            print(f\"Missing: {graph_path.name} – skipped.\")\n",
    "            continue\n",
    "\n",
    "        G = nx.read_gml(graph_path)\n",
    "        if G.number_of_nodes() == 0:\n",
    "            print(f\"Empty graph: {graph_path.name} – skipped.\")\n",
    "            continue\n",
    "\n",
    "        # Compute node strength\n",
    "        strength_dict = dict(G.degree(weight=\"weight\"))\n",
    "        strengths = list(strength_dict.values())\n",
    "\n",
    "        # Only proceed if strengths are not empty\n",
    "        if not strengths:\n",
    "            print(f\"No strengths computed for: {graph_path.name} – skipped.\")\n",
    "            continue\n",
    "\n",
    "        summary = {\n",
    "            \"repository\": repo,\n",
    "            \"phase\": phase,\n",
    "            \"num_nodes\": G.number_of_nodes(),\n",
    "            \"num_edges\": G.number_of_edges(),\n",
    "            \"mean_strength\": round(pd.Series(strengths).mean(), 2),\n",
    "            \"median_strength\": round(pd.Series(strengths).median(), 2),\n",
    "            \"max_strength\": round(pd.Series(strengths).max(), 2),\n",
    "            \"std_strength\": round(pd.Series(strengths).std(), 2),\n",
    "        }\n",
    "        strength_summary.append(summary)\n",
    "\n",
    "# Convert and sort\n",
    "strength_df = pd.DataFrame(strength_summary)\n",
    "\n",
    "if not strength_df.empty:\n",
    "    strength_df = strength_df.sort_values(by=[\"repository\", \"phase\"])\n",
    "    print(strength_df)\n",
    "\n",
    "else:\n",
    "    print(\"No data collected. Please check the input files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
