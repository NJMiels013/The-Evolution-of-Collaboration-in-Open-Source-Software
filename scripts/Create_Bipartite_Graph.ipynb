{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0affeae5",
   "metadata": {},
   "source": [
    "Section 1: Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7266d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from networkx.algorithms import bipartite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcea32",
   "metadata": {},
   "source": [
    "Section 2: Load the Cleaned PR Data per Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383642a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]\n",
    "\n",
    "# Load per-repo data\n",
    "repo_data = {}\n",
    "\n",
    "for repo in REPOS:\n",
    "    df = pd.read_csv(f\"../data/processed/{repo}_cleaned_prs.csv\", parse_dates=[\"created_at\", \"closed_at\", \"merged_at\"])\n",
    "    df[\"all_contributors\"] = df[\"all_contributors\"].apply(eval)  # Convert stringified lists back to lists\n",
    "    repo_data[repo] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7fbc1b",
   "metadata": {},
   "source": [
    "Section 3: Create the Bipartite Graphs per Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ccfab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: scikit-learn_pre_contributor_graph.gml with 438 nodes and 565 edges\n",
      "Saved: scikit-learn_during_contributor_graph.gml with 644 nodes and 817 edges\n",
      "Saved: scikit-learn_post_contributor_graph.gml with 88 nodes and 138 edges\n",
      "Saved: pytorch_pre_contributor_graph.gml with 1124 nodes and 1909 edges\n",
      "Saved: pytorch_during_contributor_graph.gml with 2919 nodes and 5419 edges\n",
      "Saved: pytorch_post_contributor_graph.gml with 941 nodes and 1604 edges\n",
      "Saved: kubernetes_pre_contributor_graph.gml with 2460 nodes and 7816 edges\n",
      "Saved: kubernetes_during_contributor_graph.gml with 2911 nodes and 7732 edges\n",
      "Saved: kubernetes_post_contributor_graph.gml with 609 nodes and 1219 edges\n",
      "Saved: apache-spark_pre_contributor_graph.gml with 470 nodes and 837 edges\n",
      "Saved: apache-spark_during_contributor_graph.gml with 665 nodes and 945 edges\n",
      "Saved: apache-spark_post_contributor_graph.gml with 211 nodes and 292 edges\n"
     ]
    }
   ],
   "source": [
    "bipartite_graphs = defaultdict(dict)\n",
    "projected_graphs = defaultdict(dict)\n",
    "\n",
    "for repo, df in repo_data.items():\n",
    "    for phase in [\"pre\", \"during\", \"post\"]:\n",
    "        df_phase = df[df[\"covid_phase\"] == phase]\n",
    "\n",
    "        B = nx.Graph()\n",
    "        for _, row in df_phase.iterrows():\n",
    "            pr_id = f\"{repo}_PR_{row['pr_number']}\"\n",
    "            contributors = row[\"all_contributors\"] or []\n",
    "\n",
    "            B.add_node(pr_id, bipartite=0)\n",
    "            for user in contributors:\n",
    "                if pd.notnull(user):\n",
    "                    B.add_node(user, bipartite=1)\n",
    "                    B.add_edge(user, pr_id)\n",
    "\n",
    "        # Store bipartite graph\n",
    "        bipartite_graphs[repo][phase] = B\n",
    "\n",
    "        # --- Project to contributor-contributor graph ---\n",
    "        contributors = {n for n, d in B.nodes(data=True) if d[\"bipartite\"] == 1}\n",
    "        G = bipartite.weighted_projected_graph(B, contributors)\n",
    "\n",
    "        # --- Filter edges: only retain those with weight > 1 ---\n",
    "        edges_to_remove = [(u, v) for u, v, d in G.edges(data=True) if d[\"weight\"] <= 1]\n",
    "        G.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Remove isolated nodes\n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "        # Store projected graph\n",
    "        projected_graphs[repo][phase] = G\n",
    "\n",
    "        # --- Save graph ---\n",
    "        nx.write_gml(G, f\"../data/networks/{repo}_{phase}_contributor_graph.gml\")\n",
    "        print(f\"Saved: {repo}_{phase}_contributor_graph.gml with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f144d",
   "metadata": {},
   "source": [
    "Section 4: Save Bipartite Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59dadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/networks/bipartite\", exist_ok=True)\n",
    "\n",
    "for repo, phases in bipartite_graphs.items():\n",
    "    for phase, graph in phases.items():\n",
    "        path = f\"../data/networks/bipartite/{repo}_{phase}_bipartite.gml\"\n",
    "        nx.write_gml(graph, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b36710",
   "metadata": {},
   "source": [
    "Section 5: Calculate and Save Netwrok Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154fb897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository</th>\n",
       "      <th>phase</th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>avg_degree</th>\n",
       "      <th>density</th>\n",
       "      <th>clustering</th>\n",
       "      <th>largest_cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>pre</td>\n",
       "      <td>438</td>\n",
       "      <td>565</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>during</td>\n",
       "      <td>644</td>\n",
       "      <td>817</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scikit-learn</td>\n",
       "      <td>post</td>\n",
       "      <td>88</td>\n",
       "      <td>138</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.3237</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>pre</td>\n",
       "      <td>1124</td>\n",
       "      <td>1909</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>during</td>\n",
       "      <td>2919</td>\n",
       "      <td>5419</td>\n",
       "      <td>3.71</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.3243</td>\n",
       "      <td>2917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pytorch</td>\n",
       "      <td>post</td>\n",
       "      <td>941</td>\n",
       "      <td>1604</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.3606</td>\n",
       "      <td>939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kubernetes</td>\n",
       "      <td>pre</td>\n",
       "      <td>2460</td>\n",
       "      <td>7816</td>\n",
       "      <td>6.35</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.3720</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kubernetes</td>\n",
       "      <td>during</td>\n",
       "      <td>2911</td>\n",
       "      <td>7732</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kubernetes</td>\n",
       "      <td>post</td>\n",
       "      <td>609</td>\n",
       "      <td>1219</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apache-spark</td>\n",
       "      <td>pre</td>\n",
       "      <td>470</td>\n",
       "      <td>837</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apache-spark</td>\n",
       "      <td>during</td>\n",
       "      <td>665</td>\n",
       "      <td>945</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>apache-spark</td>\n",
       "      <td>post</td>\n",
       "      <td>211</td>\n",
       "      <td>292</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      repository   phase  nodes  edges  avg_degree  density  clustering  \\\n",
       "0   scikit-learn     pre    438    565        2.58   0.0059      0.1871   \n",
       "1   scikit-learn  during    644    817        2.54   0.0039      0.1742   \n",
       "2   scikit-learn    post     88    138        3.14   0.0361      0.3237   \n",
       "3        pytorch     pre   1124   1909        3.40   0.0030      0.2782   \n",
       "4        pytorch  during   2919   5419        3.71   0.0013      0.3243   \n",
       "5        pytorch    post    941   1604        3.41   0.0036      0.3606   \n",
       "6     kubernetes     pre   2460   7816        6.35   0.0026      0.3720   \n",
       "7     kubernetes  during   2911   7732        5.31   0.0018      0.3612   \n",
       "8     kubernetes    post    609   1219        4.00   0.0066      0.3773   \n",
       "9   apache-spark     pre    470    837        3.56   0.0076      0.2941   \n",
       "10  apache-spark  during    665    945        2.84   0.0043      0.2375   \n",
       "11  apache-spark    post    211    292        2.77   0.0132      0.2587   \n",
       "\n",
       "    largest_cc  \n",
       "0          438  \n",
       "1          644  \n",
       "2           88  \n",
       "3         1122  \n",
       "4         2917  \n",
       "5          939  \n",
       "6         2432  \n",
       "7         2856  \n",
       "8          587  \n",
       "9          470  \n",
       "10         665  \n",
       "11         211  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        density = nx.density(G)\n",
    "        avg_deg = sum(dict(G.degree()).values()) / num_nodes if num_nodes > 0 else 0\n",
    "        clustering = nx.average_clustering(G) if num_nodes > 1 else 0\n",
    "        largest_cc = len(max(nx.connected_components(G), key=len)) if num_nodes > 0 else 0\n",
    "\n",
    "        metrics.append({\n",
    "            \"repository\": repo,\n",
    "            \"phase\": phase,\n",
    "            \"nodes\": num_nodes,\n",
    "            \"edges\": num_edges,\n",
    "            \"avg_degree\": round(avg_deg, 2),\n",
    "            \"density\": round(density, 4),\n",
    "            \"clustering\": round(clustering, 4),\n",
    "            \"largest_cc\": largest_cc\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../data/processed/network_metrics.csv\", index=False)\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7111d9c",
   "metadata": {},
   "source": [
    "Section 7: Visualize Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1052419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped scikit-learn – pre, too large to visualize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20225118\\AppData\\Local\\Temp\\ipykernel_19564\\491407180.py:20: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped scikit-learn – post, too large to visualize.\n",
      "Skipped apache-spark – pre, too large to visualize.\n",
      "Skipped apache-spark – post, too large to visualize.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "os.makedirs(\"../figures/networks\", exist_ok=True)\n",
    "\n",
    "for repo in [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]:\n",
    "    for phase in [\"pre\", \"during\", \"post\"]:\n",
    "        G = projected_graphs[repo][phase]\n",
    "        if G.number_of_nodes() == 0 or G.number_of_nodes() > 500:\n",
    "            continue  # skip too small or too large to draw clearly\n",
    "        else:\n",
    "            print(f\"Skipped {repo} – {phase}, too large to visualize.\")\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pos = nx.spring_layout(G, k=0.1)\n",
    "        nx.draw(G, pos, node_size=30, alpha=0.7, with_labels=False)\n",
    "        plt.title(f\"{repo.upper()} – {phase.capitalize()} Contributor Network\", fontsize=14, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../figures/networks/{repo}_{phase}_network.png\")\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80253d7",
   "metadata": {},
   "source": [
    "### Section 7.5: Summary of Constructed Networks\n",
    "\n",
    "Each projected contributor graph (per repository and COVID phase) includes only developers who collaborated on more than one PR. Isolated developers were removed. The resulting networks range in size, with the largest being PyTorch during COVID (2919 nodes, 5419 edges), and the smallest being scikit-learn post-COVID (89 nodes, 139 edges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca06958",
   "metadata": {},
   "source": [
    "Section 8: Centrality Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc75807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20225118\\AppData\\Local\\Temp\\ipykernel_19564\\2245875763.py:17: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\20225118\\AppData\\Local\\Temp\\ipykernel_19564\\2245875763.py:17: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Degree centrality for projected graphs\n",
    "for repo in [\"scikit-learn\", \"pytorch\", \"kubernetes\", \"apache-spark\"]:\n",
    "    for phase in [\"pre\", \"during\", \"post\"]:\n",
    "        G = projected_graphs[repo][phase]\n",
    "        if G.number_of_nodes() == 0:\n",
    "            continue\n",
    "\n",
    "        # Centrality metric (e.g. degree centrality)\n",
    "        centrality = nx.degree_centrality(G)\n",
    "        node_sizes = [500 * centrality[n] for n in G.nodes()]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        nx.draw_spring(G, node_size=node_sizes, alpha=0.7, with_labels=False)\n",
    "        plt.title(f\"{repo.upper()} – {phase.capitalize()} (Degree Centrality)\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../figures/networks/{repo}_{phase}_centrality.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818417a2",
   "metadata": {},
   "source": [
    "Section 9: Centrality Metrics per Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa42557",
   "metadata": {},
   "source": [
    "9.1: Node-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8735e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_data = []\n",
    "\n",
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        if len(G) == 0:\n",
    "            continue\n",
    "        deg = dict(G.degree())\n",
    "        strength = dict(G.degree(weight='weight'))\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "\n",
    "        for node in G.nodes():\n",
    "            centrality_data.append({\n",
    "                \"repository\": repo,\n",
    "                \"phase\": phase,\n",
    "                \"node\": node,\n",
    "                \"degree\": deg.get(node, 0),\n",
    "                \"strength\": strength.get(node, 0),\n",
    "                \"betweenness\": betweenness.get(node, 0),\n",
    "                \"eigenvector\": eigenvector.get(node, 0)\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0482a",
   "metadata": {},
   "source": [
    "9.2: Modularity and Community Detection  \n",
    "Using Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423c5c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn-pre: modularity = 0.0797, communities = 7\n",
      "scikit-learn-during: modularity = 0.0525, communities = 12\n",
      "scikit-learn-post: modularity = 0.0283, communities = 3\n",
      "pytorch-pre: modularity = 0.0725, communities = 19\n",
      "pytorch-during: modularity = 0.1323, communities = 31\n",
      "pytorch-post: modularity = 0.1328, communities = 19\n",
      "kubernetes-pre: modularity = 0.5667, communities = 31\n",
      "kubernetes-during: modularity = 0.5735, communities = 43\n",
      "kubernetes-post: modularity = 0.5937, communities = 22\n",
      "apache-spark-pre: modularity = 0.1493, communities = 6\n",
      "apache-spark-during: modularity = 0.0580, communities = 11\n",
      "apache-spark-post: modularity = 0.1165, communities = 8\n"
     ]
    }
   ],
   "source": [
    "import community as community_louvain  # pip install python-louvain\n",
    "\n",
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        if len(G) < 2:\n",
    "            continue\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        modularity = community_louvain.modularity(partition, G)\n",
    "\n",
    "        print(f\"{repo}-{phase}: modularity = {modularity:.4f}, communities = {len(set(partition.values()))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8cc83",
   "metadata": {},
   "source": [
    "9.3: Assortativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94c9f7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn-pre assortativity: -0.6535\n",
      "scikit-learn-during assortativity: -0.6665\n",
      "scikit-learn-post assortativity: -0.5660\n",
      "pytorch-pre assortativity: -0.4355\n",
      "pytorch-during assortativity: -0.3722\n",
      "pytorch-post assortativity: -0.4207\n",
      "kubernetes-pre assortativity: -0.1138\n",
      "kubernetes-during assortativity: -0.1312\n",
      "kubernetes-post assortativity: -0.2025\n",
      "apache-spark-pre assortativity: -0.4324\n",
      "apache-spark-during assortativity: -0.5539\n",
      "apache-spark-post assortativity: -0.5858\n"
     ]
    }
   ],
   "source": [
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        if len(G) > 1:\n",
    "            assort = nx.degree_assortativity_coefficient(G)\n",
    "            print(f\"{repo}-{phase} assortativity: {assort:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddce5b5",
   "metadata": {},
   "source": [
    "9.4 Turnover Metrics  \n",
    "  Based on edge/node overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506a36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn - Pre↔During Node: 0.07, Edge: 0.04\n",
      "scikit-learn - During↔Post Node: 0.05, Edge: 0.03\n",
      "pytorch - Pre↔During Node: 0.09, Edge: 0.05\n",
      "pytorch - During↔Post Node: 0.13, Edge: 0.07\n",
      "kubernetes - Pre↔During Node: 0.17, Edge: 0.05\n",
      "kubernetes - During↔Post Node: 0.11, Edge: 0.03\n",
      "apache-spark - Pre↔During Node: 0.15, Edge: 0.08\n",
      "apache-spark - During↔Post Node: 0.17, Edge: 0.09\n"
     ]
    }
   ],
   "source": [
    "def calculate_overlap(G1, G2):\n",
    "    nodes_1, nodes_2 = set(G1.nodes()), set(G2.nodes())\n",
    "    edges_1, edges_2 = set(G1.edges()), set(G2.edges())\n",
    "\n",
    "    node_overlap = len(nodes_1 & nodes_2) / len(nodes_1 | nodes_2)\n",
    "    edge_overlap = len(edges_1 & edges_2) / len(edges_1 | edges_2)\n",
    "    return node_overlap, edge_overlap\n",
    "\n",
    "for repo in REPOS:\n",
    "    try:\n",
    "        G_pre = projected_graphs[repo][\"pre\"]\n",
    "        G_during = projected_graphs[repo][\"during\"]\n",
    "        G_post = projected_graphs[repo][\"post\"]\n",
    "\n",
    "        nd, ed = calculate_overlap(G_pre, G_during)\n",
    "        dp, ep = calculate_overlap(G_during, G_post)\n",
    "        print(f\"{repo} - Pre↔During Node: {nd:.2f}, Edge: {ed:.2f}\")\n",
    "        print(f\"{repo} - During↔Post Node: {dp:.2f}, Edge: {ep:.2f}\")\n",
    "    except KeyError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df34a6",
   "metadata": {},
   "source": [
    "Section 10: Save Centrality Metrics Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fe30f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save centrality metrics\n",
    "centrality_df = pd.DataFrame(centrality_data)\n",
    "centrality_df.to_csv(\"../data/processed/centrality_metrics.csv\", index=False)\n",
    "\n",
    "# Save modularity + community count\n",
    "modularity_records = []\n",
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        if len(G) < 2:\n",
    "            continue\n",
    "        partition = community_louvain.best_partition(G)\n",
    "        modularity = community_louvain.modularity(partition, G)\n",
    "        n_communities = len(set(partition.values()))\n",
    "        modularity_records.append({\n",
    "            \"repository\": repo,\n",
    "            \"phase\": phase,\n",
    "            \"modularity\": round(modularity, 4),\n",
    "            \"num_communities\": n_communities\n",
    "        })\n",
    "\n",
    "modularity_df = pd.DataFrame(modularity_records)\n",
    "modularity_df.to_csv(\"../data/processed/modularity_metrics.csv\", index=False)\n",
    "\n",
    "# Save assortativity metrics\n",
    "assortativity_records = []\n",
    "for repo, phases in projected_graphs.items():\n",
    "    for phase, G in phases.items():\n",
    "        if G.number_of_nodes() > 1:\n",
    "            assort = nx.degree_assortativity_coefficient(G)\n",
    "            assortativity_records.append({\n",
    "                \"repository\": repo,\n",
    "                \"phase\": phase,\n",
    "                \"assortativity\": round(assort, 4)\n",
    "            })\n",
    "\n",
    "assortativity_df = pd.DataFrame(assortativity_records)\n",
    "assortativity_df.to_csv(\"../data/processed/assortativity_metrics.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
